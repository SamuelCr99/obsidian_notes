# Exam Advanced Algorithms
Samuel Collier Ryder
collier@chalmers.se
Computer Science - Algorithms, Languages and Logic (Year 1)

## Introduction
We are given a complete weighted graph $G$. The goal is to place the nodes in the graph onto a x-y coordinate system, in such a way to minimize the weighted Manhattan distance. This problem is known to be NP-complete. The goal of this exam will be to consider a number of different approximation algorithms, to try to understand which types of approximation algorithms fit this problem well. I will also be comparing the approximation ratios and time complexities between different algorithms. One might be interested in algorithms which are very efficient despite their approximation ratio not being the best. The approximation ratios will be calculated using the following formula: $$\frac{weighted \space Manhattan \space distance \space using \space approximation \space algorithm}{lower \space bound \space for \space weighted \space Manhattan \space distance \space }$$
## Lower Bound for Weighted Manhattan Distance
We need some lower bound on the weighted Manhattan distance in order to calculate the approximation ratio. One could use the sum of edge weights, as any optimal solution would always have a weighted Manhattan distance equal to or higher than this sum. In other words, assuming that the distance between all pairs of nodes equals 1. The issue with this bound is that it is very optimistic, in any solution with more than 2 nodes at least one pair of nodes would have a distance between them greater than 1. The formula for this lower bound would be: $$\sum_{i<j} w_{ij}$$
Let's try to find a more accurate lower bound. We remind ourselves that the total number of edges in the graph will be $n \choose 2$$= \frac{n*(n-1)}{2}$ We know that at most 4 nodes can be of distance 1 from any given node. This would be the case for node 8 in the image below, further more the highest number of nodes which could exist of distance $d$ from a node will always be $d*4$. For example, there will at most exist 12 nodes of distance 3 from any given node. We can use this fact to calculate a better lower bound for our approximation ratio. We must not forget that a edge is shared by two nodes, meaning that in the graph G we assume that at most $n*2$ nodes are of distance 1 from each other. So the idea behind finding this lower bound will be to assume that the $n*2$ heaviest edges will have a distance of 1, then the following $2*n*2$ heaviest edges a distance of 2, and so on. More mathematically we will have a set $D$, where $|D| = |E|$. $D_{ij} = 1$ for the $n*2$ heaviest weights, $D_{ij} = 2$ for the $2*n*2$ next heaviest weights, this continues until all $D_{ij}$ have a value. From this we finally get the following lower bound: $$\sum_{i<j} d_{ij} * w_{ij}$$ 
## Basic Algorithm
Let's first get a feel for the problem by creating a very basic approximation algorithm. Allocate $ceil(\sqrt{n})$ by $ceil(\sqrt{n})$ grid points for S. Add nodes in order of their index. While adding the nodes, add them in a way such that a square is created, see the image below for a visualization of the order nodes are added.  
![[home_exam_im3.png]]
In each step choose the next node in the list, and place it in the appropriate location, to continue building the square. Continue this process until all nodes have been placed. So what would be the worst case when using an algorithm like this? We imagine an optimal solution, this optimal solution will have a mapping for each node to a grid point. For this mapping we now consider only two nodes, imagine that these two nodes are placed next to each other in the optimal solution, what is the largest distance these two nodes could be placed from each other in the solution given by this algorithm? Remember that we have added all the nodes in a way to create a square. This means that the largest possible Manhattan distance between these two points will be $(\sqrt{n}-1)*2$. This would happen when the nodes are placed in opposite corners from each other, so node 0 and 15 or 9 and 12 from the example above. 

To calculate the approximation ratio we will use the following formula where we denote the selected x-coordinates as $x2$ and the selected y-coordinates as $y2$: $$\frac{\sum_{i<j}w_{ij}*(|x2_i-x2_j|+|y2_i-y2_j|)}{\sum_{i<j} d_{ij} * w_{ij}}$$
From the argumentation above we know the maximum distance between a selected point and the optimal point is $(\sqrt n -1)*2$. So we can rewrite this as $$\frac{(\sqrt n -1) * \sum_{i<j}w_{ij}*(|x_i-x_j|+|y_i-y_j|)}{\sum_{i<j} d_{ij} * w_{ij}}$$
And as $\sum_{i<j} d_{ij} * w_{ij}$ is a lower bound for the optimal minimum weighted distance, we can rewrite the fraction as:$$\frac{(\sqrt n -1) * \sum_{i<j} d_{ij} * w_{ij}}{\sum_{i<j} d_{ij} * w_{ij}} = (\sqrt n - 1) * 2$$ 
Finally, let's give an example. Assume that we have a case where one edge weight is significantly greater than all the other edge weights. This means that the placement of the two nodes with this high edge weight will be paramount. Now imagine that these nodes get placed in the worst possible position relative to each other, this would mean that the nodes are of distance $(\sqrt n - 1)*2$ away from each other. Meaning that the weighted Manhattan distance for the whole grid becomes $(\sqrt{n} -1)*2$ larger, as this one edge weight was significantly greater than all other edge weights.

The time complexity for this algorithm will be $O(n)$ as we will only need to loop through each available node and assign it to a position. 

## Greedy Algorithms 
The above result was clearly poor so let's consider a greedy approach. These solutions will perform the optimal step in each step. I will propose two different solutions and discuss which algorithm performs better. 

#### Greedy Algorithm I
For this algorithm we will use a dynamic size. Loop over all the available nodes and sum up all of its edges. Then sort the nodes by weight. The heaviest node will be of most importance, so we will wish to place this node in the center. And other nodes around it. So make place for the first node and place it at $(0,0)$ then continue dynamically adding space and adding the nodes around the center node. Always place the next heaviest node in the location such that it minimizes the distance to the heaviest node. If there are multiple positions which are equally close to the heaviest node, select the node which is closest to the second heaviest node and so on. 

Once again, let's consider the approximation ratio. When adding nodes in this fashion the graph will grow in a star shape, as we are always trying to add nodes which are as close to the center as possible. See visualization below: 
![[home_exam_im2.png|500]]
Even in this shape we know that the largest distance between any two nodes will be $(\sqrt n -1)*2$. And despite placing nodes with some relation to their weight, we still can't conclude that the distance between optimal point and the selected isn't $(\sqrt n -1)*2$. So from this we can follow the same argumentation as from the basic algorithm, and this will once again give us a approximation ratio of $(\sqrt{n}-1)*2$. And there are even cases where this algorithm performs worse than the basic algorithm. 

The time complexity for this algorithm will be $O(n^2)$. This complexity comes from the fact that we must compute the edge sum for all nodes, and as each node has $n-1$ edges the complexity for calculating the sum will become $O(n^2)$. We will then have to sort the edge sums which will take $O(n log(n))$. After this we only have to place each node which is a $O(n)$ operation. So in total: $O(n^2 + nlogn + n) = O(n^2)$. 

#### Greedy Algorithm II
We will now instead consider a different approach. This time we will allocate S with size $\sqrt n$ by $\sqrt n$. Sort all the edges by weight. Then consider one edge at a time:  
* If none of the nodes connected to the edge are added to S yet, add both nodes to S next to each other in such a way to continue building a square.  
* If one of the nodes connected to the edge is already added to S, but the other one is not. Add the new node to the position closest to the other node which has not already been taken. 
* If both nodes are already in S, do nothing. 

The approximation ratio will be improved compared to the basic algorithms seen above. We know that the heaviest edges will always have a distance of 1, as this will be the first two nodes which are placed. We denote the heaviest edge as $w_{max}$ and its edge $e_{max}$. So we use this in our approximation ratio. From here we will use the same argumentation as above and attempt to calculate: $$\frac{\sum_{i<j}w_{ij}*(|x2_i-x2_j|+|y2_i-y2_j|)}{\sum_{i<j} d_{ij} * w_{ij}}$$
We split this sum into two parts as we know the heaviest weight will always have a distance of 1: $$\frac{w_{max} + \sum_{i<j \land i,j \not = e_{max}}w_{ij}*(|x2_i-x2_j|+|y2_i-y2_j|)}{w_{max} + \sum_{i<j \land i,j \not = e_{max}} d_{ij} * w_{ij}}$$
Same as above we can assume that all other nodes are placed as far away from their optimal position as possible. This gives us the final approximation ratio of:  $$\frac{w_{max} + (\sqrt{n}-1)*2 * \sum_{i<j \land i,j \not = e_{max}}d_{ij}*w_{ij}}{w_{max} + \sum_{i<j \land i,j \not = e_{max}} d_{ij} * w_{ij}}$$

The time complexity will be similar to the above algorithm, we will not have to sum any edges but instead we will have to sort the edges. In total there will be $n\choose 2$ edges to sort, this equals $\frac{n!}{(n-2)! * 2!} = \frac{n*(n-1)}{2} = \frac{n^2-n}{2}$, this is clearly quadratic meaning that sorting theses edges will have a time complexity of $O(n^2*log(n^2)) = O(n^2*2log(n)) = O(n^2*log(n)$. Besides the sorting only inserting nodes into the graph and checking if nodes have been added remain. And as this is more efficient than the sorting the final complexity becomes  $O(n^2*log(n)$. 

## Random Algorithms 
Now let's consider some approximation ratios which use randomization in some way. For these algorithms we will use linearity of expectations to calculate the approximation ratio. 

#### Random Algorithm 1
We will construct an algorithm similar to the basic algorithm from above. Start by also assigning $ceil(\sqrt{n})$ by $ceil(\sqrt{n})$ grid coordinates as S. Now choose a random node and place it in the first position. Keep adding random nodes until all nodes are added. Clearly in the worst case this will perform exactly as poorly as the basic algorithm above, but as this is a randomization algorithm let's instead consider the expected approximation ratio. We will once again consider the distance between optimal location and the location the algorithm selects. We know that there are a total of $n$ locations to place nodes. Let's begin by looking at a visual example.   
![[home_exam_im1.png]]

In this example we have a 5x5 grid, where we wish to place nodes. We imagine the optimal position to place our node would be in $(0,0)$. The numbers in the cells show the distance from the optimal position. We know that the probability of landing on any given node is $\frac{1}{25}$. We will now calculate the expected distance using the following formula: 
$$\sum_{N} \frac{1}{25}* (|x - x'| + |y - y'|) = 4$$
So in this case the expected distance would be 4. 

We can also argue this in a more mathematical way. We still wish to calculate: $$\frac{\sum_{i<j}w_{ij}*(|x2_i-x2_j|+|y2_i-y2_j|)}{\sum_{i<j} d_{ij} * w_{ij}}$$
We use a lower bound and assume each node is placed in a position such that there exits another point in the grid which is $(\sqrt n -1)*2$. far away from it. Due to symmetry we know there will exist an equal number of nodes at distance $(\sqrt n - 1)*2 - x$ as at $x$. All of these symmetric pairs will sum to $\sqrt n -1$. For an example see the grid above. We use the same argumentation for substituting the upper sum as in basic algorithm, and rewrite it as:  $$\frac{(\sqrt n -1) * \sum_{i<j} d_{ij} * w_{ij}}{\sum_{i<j} d_{ij} * w_{ij}} = \sqrt n - 1$$
The time complexity will be $O(n)$ as we are still simply looping over all available nodes and adding them to a given x-y coordinate. 

#### Random Algorithm II
We can also combine the aspects of the random algorithms and the greedy algorithm. To do this we create a algorithm which starts in the same way Greedy algorithm 2, but after having placed down the first two nodes, it uses randomization like random algorithm 1 to decide which node to place down next. This will slightly improve the approximation ratio to:
$$\frac{w_{max} + (\sqrt{n}-1)* \sum_{i<j \land i,j \not = e_{max}}d_{ij}*w_{ij}}{w_{max} + \sum_{i<j \land i,j \not = e_{max}} d_{ij} * w_{ij}}$$
The reason for this result is the same as for Greedy Algorithm II and Random Algorithm 1. We know we will get a distance of 1 when placing the first two nodes, and after that we have an expected distance of $\sqrt n -1$ when placing each subsequent node. Though here it must be argued that Greedy Algorithm II is more likely a better algorithm, as it will place more node pairs closer to each other, despite the fact that Random Algorithm II performs better according to the approximation ratio. 
## Branching Algorithms
Finally let's also consider some branching algorithms. These algorithms will not be polynomial, but will hopefully have a much better approximation ratio. 

#### Branching Algorithm I
Start by considering what size of $S$ is appropriate here. In previous approximations we have used a size of either $\sqrt{n}$ by  $\sqrt{n}$. But we can't be sure that any of these will contain the optimal solution. We will instead consider a size of $n$ by $n$ which guarantees enough space for the optimal solution. The reason for this is that any larger size would only create unnecessary gaps. We now begin with our bounded search tree. The root of the tree will the empty grid. We will consider one node for each level of the tree. From the root we will add $n^2$ children, each representing placing the first node in all possible positions in S. All of these children will then get $n^2$ children of their own, which represents placing the second node in all possible positions (except the position where the first node has already been placed). This will continue until all nodes are placed. This tree will have a depth of $n$ where each node (except the leaves) have $n^2$ nodes. So there will be ${n^2}^n$ leaves. We will finally loop through each of these leaves and calculate the weighted Manhattan distance for that layout. This will have a time complexity of $O^*(n^{2n})$ which is very bad, but we are at least sure that we have found an optimal solution as we have tried every possible combination of the grid. 

#### Branching Algorithm II
We can improve the above algorithm by adding a number of bounds. First of all we can perform one of the algorithms from above. We know that the result will not be optimal but this will give us an upper bound. After performing each branching operation we will calculate the weighted Manhattan distance, and if it exceeds the previously calculated upper bound we can terminate this branch. This will mean that in each search iteration many of the potential children will not be considered. Beyond this we could also check the maximum distance between the recently added node and all other available nodes. If any node is further away than $(\sqrt n - 1)*2$ we can also terminate this branch. Using both of these bounds we will reduce the number of children each node receives. This will increase the performance of the algorithm while still making sure the result is accurate. 

## Conclusion
First of all we can conclude that using randomization works very well for the approximation algorithms. We saw the time complexity improve with a factor of 2 for both the basic and greedy solution. When looking at Greedy solutions we can conclude that looking at heavy nodes by summing edge weights does not work well. This result also makes sense as the sum of edges does not tell us a lot about which nodes should be close together. And while the greedy solution placing down heaviest edges first did not yield improve the approximation ratio massively it did at least improve it somewhat. Finally branching algorithms are time inefficient, but can be made better by adding some bounds. But at least deliver a good result. 
